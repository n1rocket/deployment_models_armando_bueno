{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFjmfnF5ER_F"
      },
      "source": [
        "# Despliegue de modelos para inferencia online\n",
        "\n",
        "En esta primera práctica vamos a aprender cómo desplegar un modelo para realizar inferencias online usando un microservicio. Para ello, utilizaremos Google Cloud Platform (GCP).\n",
        "\n",
        "El objetivo de esta práctica es dado un modelo ya entrenado, construir un microservicio capaz de disponibilizar nuestro modelo a gran escala para peticiones en tiempo real.\n",
        "\n",
        "El modelo lo tendremos almacenado en Google Cloud Storage y generaremos nuestro microservicio usando FastAPI y lo desplegaremos un el servicio Serverless de GCP, Cloud Run.\n",
        "\n",
        "Al terminar esta práctica seremos capaces de crear microservicios de Machine Learning para poner en inferencia online los modelos que deseemos a gran escala.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm4ap2ciFlu1"
      },
      "source": [
        "# Para empezar... ¿Qué es una API?\n",
        "\n",
        "Para construir aplicaciones que sean escalables e interactivas, es necesario que éstas sean capaces de comunicarse entre ellas. Por tanto, una API (abreviatura de Application Programming Interface) son una serie de reglas que facilitan las comunicaciones entre aplicaciones. Estas aplicaciones pueden ser librerías de Python o servidores web entre otros.\n",
        "\n",
        "Una de las principales ventajas de una API es que el solicitante no necesita saber el funcionamiento interno de la aplicación ni el lenguaje en el que esté desarrollado para poder responder y viceversa. Esto permite que diferentes servicios que usen diferentes tecnologías se comuniquen de una manera estándar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lPC6gNfFuIo"
      },
      "source": [
        "# Introducción a FastAPI\n",
        "\n",
        "[FastAPI](https://fastapi.tiangolo.com/) es un framework web de alto rendimiento para la construcción de APIs en Python 3.6+. Es uno de los frameworks más completos para el uso de APIs en producción vía Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgjlQkgZJZOd",
        "outputId": "7a1b6229-4563-4345-8fa8-542b4888b67e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fastapi[all]\n",
            "  Downloading fastapi-0.94.1-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 KB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyngrok\n",
            "  Downloading pyngrok-5.2.1.tar.gz (761 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m761.3/761.3 KB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.20.0-py2.py3-none-any.whl (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /usr/local/lib/python3.9/dist-packages (from fastapi[all]) (1.10.6)\n",
            "Collecting starlette<0.27.0,>=0.26.1\n",
            "  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 KB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.12.0\n",
            "  Downloading uvicorn-0.21.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 KB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.2 in /usr/local/lib/python3.9/dist-packages (from fastapi[all]) (3.1.2)\n",
            "Collecting orjson>=3.2.1\n",
            "  Downloading orjson-3.8.7-cp39-cp39-manylinux_2_28_x86_64.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.9/140.9 KB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email-validator>=1.1.1\n",
            "  Downloading email_validator-1.3.1-py2.py3-none-any.whl (22 kB)\n",
            "Collecting httpx>=0.23.0\n",
            "  Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 KB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1\n",
            "  Downloading ujson-5.7.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 KB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-multipart>=0.0.5\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 KB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: itsdangerous>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from fastapi[all]) (2.1.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.9/dist-packages (from fastapi[all]) (6.0)\n",
            "Collecting pydeck>=0.1.dev5\n",
            "  Downloading pydeck-0.8.0-py2.py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf<4,>=3.12 in /usr/local/lib/python3.9/dist-packages (from streamlit) (3.19.6)\n",
            "Requirement already satisfied: tornado>=6.0.3 in /usr/local/lib/python3.9/dist-packages (from streamlit) (6.2)\n",
            "Collecting gitpython!=3.1.19\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (5.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from streamlit) (1.22.4)\n",
            "Collecting pympler>=0.9\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 KB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (8.4.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (8.1.3)\n",
            "Collecting validators>=0.2\n",
            "  Downloading validators-0.20.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.9/dist-packages (from streamlit) (6.0.0)\n",
            "Collecting semver\n",
            "  Downloading semver-2.13.0-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.9/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from streamlit) (2.8.2)\n",
            "Requirement already satisfied: pyarrow>=4.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (4.5.0)\n",
            "Requirement already satisfied: tzlocal>=1.1 in /usr/local/lib/python3.9/dist-packages (from streamlit) (4.2)\n",
            "Requirement already satisfied: packaging>=14.1 in /usr/local/lib/python3.9/dist-packages (from streamlit) (23.0)\n",
            "Requirement already satisfied: altair<5,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from streamlit) (4.2.2)\n",
            "Collecting rich>=10.11.0\n",
            "  Downloading rich-13.3.2-py3-none-any.whl (238 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.7/238.7 KB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas<2,>=0.25 in /usr/local/lib/python3.9/dist-packages (from streamlit) (1.4.4)\n",
            "Collecting watchdog\n",
            "  Downloading watchdog-2.3.1-py3-none-manylinux2014_x86_64.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 KB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting blinker>=1.0.0\n",
            "  Downloading blinker-1.5-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: requests>=2.4 in /usr/local/lib/python3.9/dist-packages (from streamlit) (2.25.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.9/dist-packages (from altair<5,>=3.2.0->streamlit) (0.4)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.9/dist-packages (from altair<5,>=3.2.0->streamlit) (0.12.0)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.9/dist-packages (from altair<5,>=3.2.0->streamlit) (4.3.3)\n",
            "Requirement already satisfied: dnspython>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from email-validator>=1.1.1->fastapi[all]) (2.3.0)\n",
            "Requirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from email-validator>=1.1.1->fastapi[all]) (2.10)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sniffio\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting httpcore<0.17.0,>=0.15.0\n",
            "  Downloading httpcore-0.16.3-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from httpx>=0.23.0->fastapi[all]) (2022.12.7)\n",
            "Collecting rfc3986[idna2008]<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=1.4->streamlit) (3.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2>=2.11.2->fastapi[all]) (2.1.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas<2,>=0.25->streamlit) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil->streamlit) (1.15.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.4->streamlit) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.4->streamlit) (1.26.15)\n",
            "Collecting markdown-it-py<3.0.0,>=2.2.0\n",
            "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygments<3.0.0,>=2.13.0\n",
            "  Downloading Pygments-2.14.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting anyio<5,>=3.4.0\n",
            "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.9/dist-packages (from tzlocal>=1.1->streamlit) (0.1.0.post0)\n",
            "Collecting h11>=0.8\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting websockets>=10.4\n",
            "  Downloading websockets-10.4-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 KB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0\n",
            "  Downloading uvloop-0.17.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0\n",
            "  Downloading httptools-0.5.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (417 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 KB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13\n",
            "  Downloading watchfiles-0.18.1-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.9/dist-packages (from validators>=0.2->streamlit) (4.4.2)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair<5,>=3.2.0->streamlit) (22.2.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair<5,>=3.2.0->streamlit) (0.19.3)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.9/dist-packages (from pytz-deprecation-shim->tzlocal>=1.1->streamlit) (2022.7)\n",
            "Building wheels for collected packages: pyngrok, validators\n",
            "  Building wheel for pyngrok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyngrok: filename=pyngrok-5.2.1-py3-none-any.whl size=19790 sha256=f718af140df589d43f1dc03722320ec0a61d49bfde3effa00b53cb014b4251a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/89/59/49d4249e00957e94813ac136a335d10ed2e09a856c5096f95c\n",
            "  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for validators: filename=validators-0.20.0-py3-none-any.whl size=19581 sha256=0b7e13c30a9c00b5b693bd6e2aa02d276f477d2481a81848e1a4c8b24d134186\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/f0/a8/1094fca7a7e5d0d12ff56e0c64675d72aa5cc81a5fc200e849\n",
            "Successfully built pyngrok validators\n",
            "Installing collected packages: rfc3986, websockets, watchdog, validators, uvloop, ujson, sniffio, smmap, semver, python-multipart, python-dotenv, pyngrok, pympler, pygments, orjson, mdurl, httptools, h11, email-validator, blinker, uvicorn, pydeck, markdown-it-py, gitdb, anyio, watchfiles, starlette, rich, httpcore, gitpython, streamlit, httpx, fastapi\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.6.1\n",
            "    Uninstalling Pygments-2.6.1:\n",
            "      Successfully uninstalled Pygments-2.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed anyio-3.6.2 blinker-1.5 email-validator-1.3.1 fastapi-0.94.1 gitdb-4.0.10 gitpython-3.1.31 h11-0.14.0 httpcore-0.16.3 httptools-0.5.0 httpx-0.23.3 markdown-it-py-2.2.0 mdurl-0.1.2 orjson-3.8.7 pydeck-0.8.0 pygments-2.14.0 pympler-1.0.1 pyngrok-5.2.1 python-dotenv-1.0.0 python-multipart-0.0.6 rfc3986-1.5.0 rich-13.3.2 semver-2.13.0 smmap-5.0.0 sniffio-1.3.0 starlette-0.26.1 streamlit-1.20.0 ujson-5.7.0 uvicorn-0.21.0 uvloop-0.17.0 validators-0.20.0 watchdog-2.3.1 watchfiles-0.18.1 websockets-10.4\n"
          ]
        }
      ],
      "source": [
        "! pip install fastapi[all] pyngrok streamlit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x50jhB4qezZS",
        "outputId": "e7da660a-a677-4320-89e3-488bd86f6eb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nest_asyncio\n",
            "  Downloading nest_asyncio-1.5.6-py3-none-any.whl (5.2 kB)\n",
            "Installing collected packages: nest_asyncio\n",
            "Successfully installed nest_asyncio-1.5.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ole4kY64LX50",
        "outputId": "460316cd-1ced-4330-fd5e-032224764729"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile main.py\n",
        "\n",
        "from fastapi import FastAPI #importamos el modulo bascio\n",
        "\n",
        "app = FastAPI()  #creamos la aplicacion (la API) instanciando el objeto\n",
        "\n",
        "\n",
        "@app.get(\"/saluda\") #añadimos una ruta que le permite comunicarse con otros servicios. Es una cláusula para obtención de información\n",
        "async def root():\n",
        "    return {\"message\": \"Hello World\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las siguientes celdas permiten ejecutar un microservicio en colab. Si deseáis reproducirlo os tenéis que generar un token en [ngrok](https://dashboard.ngrok.com/get-started/your-authtoken)."
      ],
      "metadata": {
        "id": "q_B8IqmQd-WR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La primera línea importa FastAPI. Tras lo cual se crea una APP vacía y se añade una consulta get. Las consultas get buscan devolver información. Todo esto se escribe en un fichero. Podemos probarlo con lo siguiente:"
      ],
      "metadata": {
        "id": "YVMNbJ2r6XZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "conf.get_default().auth_token = \"\" #cada uno os lo debéis generar en ngrok.\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(8000) #8000 es el puerto en el que le indicamos que escuche\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "9kzdeUQL7GBX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28b353f6-80a9-4911-e704-e72ba2eaedd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: http://6395-35-203-159-21.ngrok.io\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esto genera un tunel de la URL pública a la interna de la máquina. Mediante la siguiente celda podemos observar la traza de las llamadas que se hacen y llamar a la API"
      ],
      "metadata": {
        "id": "pC6LNXRl7AxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! uvicorn main:app --port 8000"
      ],
      "metadata": {
        "id": "-s_tjdIl7TWG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57968d68-d044-4ba7-fb82-7fc59d58dfa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m1476\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://127.0.0.1:8000\u001b[0m (Press CTRL+C to quit)\n",
            "\u001b[32mINFO\u001b[0m:     83.32.134.84:0 - \"\u001b[1mGET / HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     83.32.134.84:0 - \"\u001b[1mGET /favicon.ico HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     83.32.134.84:0 - \"\u001b[1mGET /saluda HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     83.32.134.84:0 - \"\u001b[1mGET /favicon.ico HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     Shutting down\n",
            "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m1476\u001b[0m]\n",
            "\u001b[31mERROR\u001b[0m:    Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/starlette/routing.py\", line 686, in lifespan\n",
            "    await receive()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/uvicorn/lifespan/on.py\", line 139, in receive\n",
            "    return await self.receive_queue.get()\n",
            "  File \"/usr/lib/python3.9/asyncio/queues.py\", line 166, in get\n",
            "    await getter\n",
            "asyncio.exceptions.CancelledError\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los 404 salen porque no hay nada definido en esas rutas."
      ],
      "metadata": {
        "id": "ke509hRngHwq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si pones esa URL en el ordenador debería lanzar el mensaje. (Puede surgir un problema de seguridad, aceptamos). Una vez en la URL añadimos la / y con eso ya lo tendremos.\n",
        "\n",
        "Veamos un ejemplo algo más complejo:"
      ],
      "metadata": {
        "id": "QHSR3bYf7VUs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8N6cLqNJyMm",
        "outputId": "d225c7e3-7693-4509-b160-f491788fae1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mainpost.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile mainpost.py\n",
        "\n",
        "from typing import Optional\n",
        "\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class Indentity(BaseModel): #los identity están compuestos de un nombre obligatorio y un apellido opcional\n",
        "    name: str\n",
        "    surname: Optional[str] = None\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "@app.get(\"/saluda\")\n",
        "async def root():\n",
        "    return {\"message\": \"Hello World\"}\n",
        "\n",
        "@app.post(\"/testing\") #los metodos post reciben informacion\n",
        "async def testing(id: Indentity): #la API espera un input de tipo Identity. Si no cumple falla\n",
        "    if id.surname is None: # si no hay apellido, devuelve solo el nombre\n",
        "      message = f\"Welcome to the API! My name is {id.name}\"\n",
        "    else: message = f\"Welcome to the API! My name is {id.name} {id.surname}\" # si vienen los dos devuelven los dos\n",
        "    return {\"message\": message}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solo acepta la petición si se cumplen los requisitos de entrada, si no se rechaza directamente."
      ],
      "metadata": {
        "id": "rRwIo13j8E5E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmAYFQ-SKCN_",
        "outputId": "9ce44979-2fa4-400a-e9a7-3b5305b0f3a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: http://2be5-35-203-159-21.ngrok.io\n"
          ]
        }
      ],
      "source": [
        "import nest_asyncio\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "conf.get_default().auth_token = \"\" #cada uno os lo debéis generar en ngrok.\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Si abrimos la URL no aparece directamente. Podemos usar [Postman](https://www.postman.com/) para lanzar peticiones a la API:"
      ],
      "metadata": {
        "id": "t8A0aVKw8mWj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsEoRB6c_Jyr",
        "outputId": "5a498cb3-622c-4bcd-b4e2-42429e504c80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m3353\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://127.0.0.1:8000\u001b[0m (Press CTRL+C to quit)\n",
            "\u001b[32mINFO\u001b[0m:     83.32.134.84:0 - \"\u001b[1mGET / HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     83.32.134.84:0 - \"\u001b[1mGET /favicon.ico HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     83.32.134.84:0 - \"\u001b[1mGET /saluda HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     83.32.134.84:0 - \"\u001b[1mGET /favicon.ico HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     83.32.134.84:0 - \"\u001b[1mGET /testing HTTP/1.1\u001b[0m\" \u001b[31m405 Method Not Allowed\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     54.86.50.139:0 - \"\u001b[1mGET /saluda HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     54.86.50.139:0 - \"\u001b[1mPOST /testing HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     54.86.50.139:0 - \"\u001b[1mPOST /testing HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     54.86.50.139:0 - \"\u001b[1mPOST /testing HTTP/1.1\u001b[0m\" \u001b[31m422 Unprocessable Entity\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     Shutting down\n",
            "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m3353\u001b[0m]\n",
            "\u001b[31mERROR\u001b[0m:    Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/starlette/routing.py\", line 686, in lifespan\n",
            "    await receive()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/uvicorn/lifespan/on.py\", line 139, in receive\n",
            "    return await self.receive_queue.get()\n",
            "  File \"/usr/lib/python3.9/asyncio/queues.py\", line 166, in get\n",
            "    await getter\n",
            "asyncio.exceptions.CancelledError\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! uvicorn mainpost:app --port 8000"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_AVQiDW_drT",
        "outputId": "14665dd5-540f-44d2-eb66-f7f99343a45b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLQtYIS0kRF4"
      },
      "source": [
        "# ¡Manos a la obra!\n",
        "\n",
        "Ahora vamos a comenzar con el desarrollo de la API para inferencias online. Para cada petición que recibamos de predicción vamos a realizar tres pasos:\n",
        "\n",
        "1.   **Preprocesamiento**: en este primer paso extraeremos el dato a inferir de la petición y aplicaremos el preprocesado necesario\n",
        "2.   **Inferencia**: realizaremos la inferencia sobre nuestro modelo.\n",
        "3. **Postprocesado**: generaremos un JSON de respuesta con el resultado de la inferencia\n",
        "\n",
        "**INTRODUCIR DIAGRAMA DE LA PRÁCTICA**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrxbT-7-pt9f"
      },
      "source": [
        "## Configuración de nuestro proyecto en GCP\n",
        "\n",
        "**Los siguientes pasos es obligatorio realizarlos para seguir con la práctica.**\n",
        "\n",
        "1.   Selecciona o crea un proyecto en GCP\n",
        "2.   Asegurate de que la facturación está activada para tu proyecto.\n",
        "3.   [Habilita la API de Google Cloud Storage](https://console.cloud.google.com/apis/library/storage-component.googleapis.com?q=storage).\n",
        "4. [Habilita la API de Google Cloud Registry](https://console.cloud.google.com/apis/library/containerregistry.googleapis.com?q=container).\n",
        "5. [Habilita la API de Google Cloud Run](https://console.cloud.google.com/apis/library/run.googleapis.com?q=cloud%20run).\n",
        "6. [Habilita la API de Google Cloud Build](https://console.cloud.google.com/apis/library/cloudbuild.googleapis.com?q=cloud%20build).\n",
        "7. [Habilita la API de App Engine Flexible Environment](https://console.cloud.google.com/apis/library/appengineflex.googleapis.com?q=app%20eng).\n",
        "8. [Habilita la API de App Engine Admin](https://console.cloud.google.com/apis/library/appengine.googleapis.com?q=app%20engine).\n",
        "9. Introduce tu ID de proyecto de GCP en la celda de abajo. Ejecuta la celda para asegurarnos de que el Cloud SDK usa el proyecto adecuado para todos los comandos en este notebook.\n",
        "\n",
        "**Nota**: Jupyter ejecuta las lineas con el prefijo `!` como comandos shell de consola, y puede usar variables de Python en los comandos añadiendoles el prefijo `$`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qxwBA4RM9Lu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69e4cb6f-a8e0-43fe-c7a5-238fdd63563d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ],
      "source": [
        "PROJECT_ID = \"puestaproduccionkcmarzo23\" #@param {type:\"string\"}\n",
        "! gcloud config set project $PROJECT_ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9i6oektpgld"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import auth as google_auth\n",
        "  google_auth.authenticate_user()\n",
        "\n",
        "# If you are running this notebook locally, replace the string below with the\n",
        "# path to your service account key and run this cell to authenticate your GCP\n",
        "# account.\n",
        "else:\n",
        "  %env GOOGLE_APPLICATION_CREDENTIALS ''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azNFFNMgpKBr"
      },
      "source": [
        "## Creación bucket en Cloud Storage\n",
        "\n",
        "**Los siguientes pasos son obligatorios.**\n",
        "\n",
        "Cuando ejecutemos un job de entrenamiento usando el Cloud SDK, lo que hacemos es subir un paquete Python que contiene el código de entrenamiento a Google Cloud Storage. AI Platform ejecuta este paquete en el job.\n",
        "\n",
        "Establece el nombre del bucket a continuación. El nombre tiene que ser único para todos los bucket de GCP. También tenemos que establecer la variable `REGION`, la cual usaremos para todas las operaciones a lo largo del notebook. Asegurate de [elegir una región en la que Cloud AI Platform esté disponible](https://cloud.google.com/ml-engine/docs/tensorflow/regions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTxmbDg1I0x1"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"puestaproduccionkcmarzo23-art\" #@param {type:\"string\"}\n",
        "REGION = \"europe-west1\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlQymziRrHqS"
      },
      "source": [
        "**Sólo si tu bucket aún no existe**: Ejecuta la siguiente celda para crear tu bucket en Cloud Storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "160PRO3aJqLD"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION gs://$BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiH1VHmqrVVE"
      },
      "source": [
        "Finalmente, validamos que tenemos acceso al bucket de Cloud Storage mirando sus contenidos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsB4T3sbSb2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bb686cd-3ab0-4bec-ac28-3ba659d5f688"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 gs://puestaproduccionkcmarzo23-art/data/\n",
            "                                 gs://puestaproduccionkcmarzo23-art/twitter-sentiment-batch/\n"
          ]
        }
      ],
      "source": [
        "! gsutil ls -al gs://$BUCKET_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aparece la carpeta de batch ergo todo correcto"
      ],
      "metadata": {
        "id": "g4SPp3apjYiL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnK-kOtZrjSv"
      },
      "source": [
        "## Descarga de la plantilla de código\n",
        "\n",
        "Ahora nos descargaremos la plantilla de código que vamos a ir rellenando para el desarrollo de la práctica y establecemos el directorio como directorio de trabajo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "myt2V07Ds8sj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba7b408a-b8c8-4231-abf2-cd3aea4c984e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TwitterOnline'...\n",
            "remote: Enumerating objects: 27, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 27 (delta 1), reused 27 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (27/27), 5.87 KiB | 1001.00 KiB/s, done.\n",
            "/content/TwitterOnline\n"
          ]
        }
      ],
      "source": [
        "# Clone the repository\n",
        "! git clone https://github.com/ArturoSanchezPalacio/TwitterOnline.git\n",
        "\n",
        "# Set the working directory to the sample code directory\n",
        "%cd ./TwitterOnline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSu5B3uqk2rp",
        "outputId": "59804ab1-58a0-4b1c-fb3f-a3824658a2eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app  app.py  Dockerfile  README.md  requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIQBqjSqthMj"
      },
      "source": [
        "## Instalación de dependencias\n",
        "\n",
        "Ejecutamos la siguiente celda para instalar las dependencias de Python necesarias para entrenar el modelo localmente y preprocesar datos. \n",
        "\n",
        "Cuando ejecutemos el job de entrenamiento en AI Platform, las dependencias estarán instaladas en base a la [versión del runtime](https://cloud.google.com/ml-engine/docs/tensorflow/runtime-version-list) elegido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wm5w1UrmVU7O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3373238-73eb-4767-80db-d58bad5e58f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting requests==2.25.0\n",
            "  Downloading requests-2.25.0-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 KB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn==0.12.2\n",
            "  Downloading uvicorn-0.12.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 KB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi==0.61.2\n",
            "  Downloading fastapi-0.61.2-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 KB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h5py==2.10.0\n",
            "  Downloading h5py-2.10.0.tar.gz (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.1/301.1 KB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting scikit-learn==0.23.2\n",
            "  Downloading scikit-learn-0.23.2.tar.gz (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjcPYWRE4u45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd4a4ddd-9483-485e-ee35-3351f7106594"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project: puestaproduccionkcmarzo23\n"
          ]
        }
      ],
      "source": [
        "# Nos aseguramos que nuestras variables de entorno no hayan desaparecido si hemos reiniciado el kernel\n",
        "\n",
        "print(f\"Project: {PROJECT_ID}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EI4ez_cbGDLb"
      },
      "source": [
        "# Desarrollo del microservicio de inferencia\n",
        "\n",
        "En la plantilla de código se proporciona una estructura de proyecto genérica para cualquier desarrollo de una API de inferencia online lista para ser usada de manera productiva."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpAFp9VUkQLI"
      },
      "source": [
        "El proyecto tiene la siguiente estructura:\n",
        "\n",
        "``` bash\n",
        "twitter-sentiment-online/\n",
        "├── app/\n",
        "│   ├── __init__.py\n",
        "│   ├── api/\n",
        "│   │   ├── __init__.py\n",
        "│   │   └── routes/\n",
        "│   │       ├── __init__.py\n",
        "│   │       ├── heartbeat.py\n",
        "│   │       ├── prediction.py\n",
        "│   │       └── router.py\n",
        "│   ├── core/\n",
        "│   │   ├── __init__.py\n",
        "│   │   ├── config.py\n",
        "│   │   ├── enums.py\n",
        "│   │   ├── event_handlers.py\n",
        "│   │   └── messages.py\n",
        "│   ├── main.py\n",
        "│   ├── models/\n",
        "│   │   ├── __init__.py\n",
        "│   │   ├── heartbeat.py\n",
        "│   │   ├── payload.py\n",
        "│   │   └── prediction.py\n",
        "│   └── services/\n",
        "│       ├── __init__.py\n",
        "│       └── models.py\n",
        "├── Dockerfile\n",
        "├── README.md\n",
        "└── requirements.txt\n",
        "```\n",
        "\n",
        "En esta estructura distinguimos los siguientes componentes:\n",
        "\n",
        "* **README.md**: instrucciones de uso.\n",
        "* **Dockerfile**: aquí definimos la imagen base que usaremos y como empaquetamos el proyecto.\n",
        "* **requirements.txt**: especificación de dependencias a instalar en el microservicio. Indica lo que va a construir.\n",
        "* **app**: aplicación de inferencia online en FastAPI. Toda la aplicación que estamos desarrollando se encuentra en este fichero.\n",
        "    * **main.py**: punto de entrada para la ejecución de la aplicación. Incluye todas las rutas de la API así como los servicios utilizados por la misma.\n",
        "    * **models**: en este encontramos la definición de los esquemas que usaremos dentro de la aplicación. No hace referencia a modelos de Machine Learning sino a modelos de datos, es decir, a esquemas de la información. Sería como la clase Identity del ejemplo previo.\n",
        "    * **services**: en este módulo incluiremos la implementación de nuestra clase de inferencia. Es la parte del código más personalizada. Aquí incluiremos la carga del modelo, la generación de la predicción o el formateo de los datos de salida.\n",
        "    * **api/routes**: módulo en el que definiremos los diferentes endpoints que tendrá la API, son las rutas de acceso a la información.\n",
        "    * **api/core**: módulo donde estarán funcionalidades comunes al servicio y configuraciones. En él almacenamos funciones que se reutilizan."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FUa3ZksyWHY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65b41de0-479b-457c-896e-5f37d52b2d31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/TwitterOnline\n"
          ]
        }
      ],
      "source": [
        "%cd /content/TwitterOnline/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVuf4q6dmh4g",
        "outputId": "c5f49600-2a06-4195-f792-dba86e6087ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app  app.py  Dockerfile  README.md  requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defino la ruta en la que se encuentra el modelo que voy a usar para realizar las inferencias:"
      ],
      "metadata": {
        "id": "FiznOsWNBEgz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn9xsgx4u5xP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"DEFAULT_MODEL_PATH\"] = \"/content/TwitterOnline/\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nos bajamos los archivos que ya hemos generado previamente"
      ],
      "metadata": {
        "id": "UrfQNs42BUfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjWBKoUFi4f5",
        "outputId": "3e59b2ff-acb1-4860-c5d7-c9af7c4722de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app  app.py  Dockerfile  README.md  requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9A47h9iMuoiT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2324f6b9-e3c9-4fb4-d277-101ff1e3c623"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://puestaproduccionkcmarzo23-art/twitter-sentiment-batch/data/model/model.h5...\n",
            "/ [0/2 files][    0.0 B/355.5 MiB]   0% Done                                    \r==> NOTE: You are downloading one or more large file(s), which would\n",
            "run significantly faster if you enabled sliced object downloads. This\n",
            "feature is enabled by default but requires that compiled crcmod be\n",
            "installed (see \"gsutil help crcmod\").\n",
            "\n",
            "Copying gs://puestaproduccionkcmarzo23-art/twitter-sentiment-batch/data/model/tokenizer.pkl...\n",
            "| [2/2 files][355.5 MiB/355.5 MiB] 100% Done                                    \n",
            "Operation completed over 2 objects/355.5 MiB.                                    \n"
          ]
        }
      ],
      "source": [
        "! gsutil -m cp \\\n",
        "  \"gs://puestaproduccionkcmarzo23-art/twitter-sentiment-batch/data/model/model.h5\" \\\n",
        "  \"gs://puestaproduccionkcmarzo23-art/twitter-sentiment-batch/data/model/tokenizer.pkl\" \\\n",
        "  ."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comprobamos que están aparecen a la izquierda:"
      ],
      "metadata": {
        "id": "6CtWiiO7nB1X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora exploremos el código con el que vamos a trabajar."
      ],
      "metadata": {
        "id": "5Is4I7AxnPL5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwFsPWPivkaA"
      },
      "source": [
        "Para poder probar el correcto funcionamiento del servicio en local:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzlUWSlVA7pl",
        "outputId": "6c321c0c-6600-4b44-c232-81348dbdcaab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app  app.py  Dockerfile  model.h5  README.md  requirements.txt\ttokenizer.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncn9K4uJtsBM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "440086ab-f325-492b-8084-97d8fd1fc47d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: http://3011-104-196-115-8.ngrok.io\n"
          ]
        }
      ],
      "source": [
        "import nest_asyncio\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "conf.get_default().auth_token = \"\"\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install loguru"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jckNYVR8rbkR",
        "outputId": "cb0c10e1-babe-4399-9497-82cf4a7f3da4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting loguru\n",
            "  Downloading loguru-0.6.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: loguru\n",
            "Successfully installed loguru-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFeSh7rcuG7L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd8bdb65-2a73-449a-b99a-526c19714fa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-03-14 21:49:39.620156: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/asyncio/runners.py\", line 44, in run\n",
            "    return loop.run_until_complete(main)\n",
            "  File \"uvloop/loop.pyx\", line 1511, in uvloop.loop.Loop.run_until_complete\n",
            "  File \"uvloop/loop.pyx\", line 1504, in uvloop.loop.Loop.run_until_complete\n",
            "  File \"uvloop/loop.pyx\", line 1377, in uvloop.loop.Loop.run_forever\n",
            "  File \"uvloop/loop.pyx\", line 555, in uvloop.loop.Loop._run\n",
            "  File \"uvloop/loop.pyx\", line 474, in uvloop.loop.Loop._on_idle\n",
            "  File \"uvloop/cbhandles.pyx\", line 83, in uvloop.loop.Handle._run\n",
            "  File \"uvloop/cbhandles.pyx\", line 61, in uvloop.loop.Handle._run\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/uvicorn/server.py\", line 66, in serve\n",
            "    config.load()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/uvicorn/config.py\", line 471, in load\n",
            "    self.loaded_app = import_from_string(self.app)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/uvicorn/importer.py\", line 21, in import_from_string\n",
            "    module = importlib.import_module(module_str)\n",
            "  File \"/usr/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
            "  File \"/content/TwitterOnline/app/main.py\", line 3, in <module>\n",
            "    from app.api.routes.router import api_router\n",
            "  File \"/content/TwitterOnline/app/api/routes/router.py\", line 3, in <module>\n",
            "    from app.api.routes import heartbeat, prediction\n",
            "  File \"/content/TwitterOnline/app/api/routes/prediction.py\", line 6, in <module>\n",
            "    from app.services.models import SentimentAnalysisModel\n",
            "  File \"/content/TwitterOnline/app/services/models.py\", line 6, in <module>\n",
            "    import tensorflow as tf\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/__init__.py\", line 37, in <module>\n",
            "    from tensorflow.python.tools import module_util as _module_util\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/__init__.py\", line 42, in <module>\n",
            "    from tensorflow.python import data\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/__init__.py\", line 21, in <module>\n",
            "    from tensorflow.python.data import experimental\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/experimental/__init__.py\", line 96, in <module>\n",
            "    from tensorflow.python.data.experimental import service\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/experimental/service/__init__.py\", line 419, in <module>\n",
            "    from tensorflow.python.data.experimental.ops.data_service_ops import distribute\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/experimental/ops/data_service_ops.py\", line 22, in <module>\n",
            "    from tensorflow.python.data.experimental.ops import compression_ops\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/experimental/ops/compression_ops.py\", line 16, in <module>\n",
            "    from tensorflow.python.data.util import structure\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/util/structure.py\", line 29, in <module>\n",
            "    from tensorflow.python.ops import resource_variable_ops\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/resource_variable_ops.py\", line 38, in <module>\n",
            "    from tensorflow.python.framework import meta_graph\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/meta_graph.py\", line 34, in <module>\n",
            "    from tensorflow.python.framework import importer\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/importer.py\", line 24, in <module>\n",
            "    from tensorflow.python.framework import function\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/function.py\", line 34, in <module>\n",
            "    from tensorflow.python.ops import variable_scope as vs\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/variable_scope.py\", line 32, in <module>\n",
            "    from tensorflow.python.ops import init_ops\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/init_ops.py\", line 41, in <module>\n",
            "    from tensorflow.python.ops import linalg_ops_impl\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/linalg_ops_impl.py\", line 22, in <module>\n",
            "    from tensorflow.python.ops import math_ops\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/math_ops.py\", line 87, in <module>\n",
            "    from tensorflow.python.ops import gen_nn_ops\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 9440, in <module>\n",
            "    QuantizedDepthwiseConv2DWithBiasAndRelu = tf_export(\"raw_ops.QuantizedDepthwiseConv2DWithBiasAndRelu\")(_ops.to_raw_op(quantized_depthwise_conv2d_with_bias_and_relu))\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/framework/ops.py\", line 7210, in to_raw_op\n",
            "    return kwarg_only(f)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/tf_export.py\", line 404, in kwarg_only\n",
            "    f_argspec = tf_inspect.getargspec(f)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/tf_inspect.py\", line 140, in getargspec\n",
            "    return _getargspec(target)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/tf_inspect.py\", line 81, in _getargspec\n",
            "    fullargspecs = getfullargspec(target)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/tf_inspect.py\", line 266, in getfullargspec\n",
            "    return _getfullargspec(target)\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 1162, in getfullargspec\n",
            "    sig = _signature_from_callable(func,\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 2325, in _signature_from_callable\n",
            "    return _signature_from_function(sigcls, obj,\n",
            "  File \"/usr/lib/python3.9/inspect.py\", line 2204, in _signature_from_function\n",
            "    annotation = annotations.get(name, _empty)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.9/asyncio/runners.py\", line 49, in run\n",
            "    loop.run_until_complete(loop.shutdown_default_executor())\n",
            "  File \"uvloop/loop.pyx\", line 1511, in uvloop.loop.Loop.run_until_complete\n",
            "  File \"uvloop/loop.pyx\", line 1504, in uvloop.loop.Loop.run_until_complete\n",
            "  File \"uvloop/loop.pyx\", line 1377, in uvloop.loop.Loop.run_forever\n",
            "  File \"uvloop/loop.pyx\", line 534, in uvloop.loop.Loop._run\n",
            "  File \"uvloop/loop.pyx\", line 300, in uvloop.loop.Loop._setup_or_resume_signals\n",
            "  File \"/usr/lib/python3.9/socket.py\", line 606, in socketpair\n",
            "    a, b = _socket.socketpair(family, type, proto)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/uvicorn\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/click/core.py\", line 1130, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/click/core.py\", line 1055, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/click/core.py\", line 1404, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/click/core.py\", line 760, in invoke\n",
            "    return __callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/uvicorn/main.py\", line 403, in main\n",
            "    run(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/uvicorn/main.py\", line 568, in run\n",
            "    server.run()\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/uvicorn/server.py\", line 59, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "  File \"/usr/lib/python3.9/asyncio/runners.py\", line 52, in run\n",
            "    loop.close()\n",
            "  File \"uvloop/loop.pyx\", line 1391, in uvloop.loop.Loop.close\n",
            "  File \"uvloop/loop.pyx\", line 561, in uvloop.loop.Loop._close\n",
            "RuntimeError: Cannot close a running event loop\n"
          ]
        }
      ],
      "source": [
        "! uvicorn app.main:app --port 8000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIvEgcv_bxtK"
      },
      "source": [
        "## Creando la interfaz usando Streamlit\n",
        "\n",
        "[Streamlit](https://www.streamlit.io/) es un framework para la creación de Webapps orientado a datos e Inteligencia Artificial basado en Python. Echemos un ojo.\n",
        "\n",
        "En este caso vamos a desarrollar un pequeño frontal para poder invocar nuestro recién desarrollado servicio de inferencia para realizar predicciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGBBJ8wvso_Y"
      },
      "outputs": [],
      "source": [
        "%mkdir /content/prediction-front"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyZNJWwLpvp3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75c18202-f074-41f7-d004-432c93a296c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/prediction-front\n"
          ]
        }
      ],
      "source": [
        "%cd /content/prediction-front"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos un primer ejemplo de un front:"
      ],
      "metadata": {
        "id": "wBnc2-CmDKYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile front.py\n",
        "\n",
        "import requests\n",
        "import validators\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "\n",
        "st.title(\"Predicciones Análisis Sentimiento KeepCoding\")\n",
        "\n",
        "st.markdown(\"¡Bievenid@! Vamos a crear un front para poder ves las predicciones de nuestro modelo :smile:\") #ponemos este titulo\n",
        "\n",
        "st.write(\"Debemos introducir la URL donde se servirán las predicciones de nuestro modelo\") #escribimos este mensaje\n",
        "\n",
        "server_url = st.text_input(\"URL predicciones\", value=\"\") #vacio por defecto, si no se almacena la URL en la variable\n",
        "\n",
        "if server_url !=\"\":\n",
        "  st.write(f\"URL: {server_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrKCUv5lDMQZ",
        "outputId": "a64e331e-3ae7-4ed4-9305-ef1f4afd4739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing front.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtWgH2iZm5Hj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8201b57-5d46-4b85-d837-e5edaaa57f10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: http://59fd-104-196-115-8.ngrok.io\n"
          ]
        }
      ],
      "source": [
        "import nest_asyncio\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "conf.get_default().auth_token = \"\"\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(8501)\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYfVS1UNm6HY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5cba080-ed95-4294-8e95-66fd20e2f9ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://104.196.115.8:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "! streamlit run front.py"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos parar la URL para ver un ejemplo más completo"
      ],
      "metadata": {
        "id": "hL_n64bfFNKa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El objetivo del siguiente script es levantar un front capaz de recibir un texto y elaborar una predicción sobre nuestro modelo:"
      ],
      "metadata": {
        "id": "eL9TCyuBFNNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile front.py\n",
        "\n",
        "import requests\n",
        "import validators\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "class Session:\n",
        "    pass\n",
        "\n",
        "\n",
        "@st.cache(allow_output_mutation=True)\n",
        "def fetch_session():\n",
        "    session = Session()\n",
        "    session.url = None\n",
        "    session.predictions = None\n",
        "    return session\n",
        "\n",
        "\n",
        "session_state = fetch_session()\n",
        "\n",
        "\n",
        "def validate_url(server_url): #comprueba que el texto introducido es una URL\n",
        "    info_placeholder = st.empty()\n",
        "\n",
        "    if server_url == \"\":\n",
        "        # wait\n",
        "        info_placeholder.write(\"\")\n",
        "    elif validators.url(server_url):\n",
        "        # Save url\n",
        "        session_state.url = server_url\n",
        "        info_placeholder.write(\n",
        "            \"Perfect! Now we can start predicting. Insert your desired text below to make predictions:\"\n",
        "        )\n",
        "    else:\n",
        "        info_placeholder.write(\"Introduced text is not a URL, stopping...\")\n",
        "        st.stop()\n",
        "\n",
        "\n",
        "def predict():\n",
        "    input_text = st.text_area(\"Enter text\") #introducimos el texto\n",
        "    if input_text != \"\":\n",
        "        payload = {\"text\": input_text} #si el texto es distinto de vacío lo metemos en un diccionario\n",
        "        try:\n",
        "            response = requests.post(session_state.url, json=payload) #realizamoms la petición post a la URL\n",
        "            prediction = response.json() #extraemos la información de respuesta en un json\n",
        "            prediction[\"text\"] = input_text #metemos el texto en el json\n",
        "            if session_state.predictions is None: #si la predicción exixte la vacíamos en un dataframe de salida\n",
        "                session_state.predictions = pd.DataFrame.from_dict(\n",
        "                    {k: [v] for k, v in prediction.items()}\n",
        "                )\n",
        "            else:\n",
        "                session_state.predictions = session_state.predictions.append(\n",
        "                    {k: v for k, v in prediction.items()}, ignore_index=True\n",
        "                )\n",
        "            st.table(session_state.predictions)\n",
        "            st.balloons()\n",
        "        except Exception as ex:\n",
        "            st.error(repr(ex))\n",
        "        st.stop()\n",
        "\n",
        "\n",
        "st.title(\"Sentiment Analysis Predictions\")\n",
        "\n",
        "st.markdown(\n",
        "    \"Welcome! With this app you can predict the sentiment of a given text using Deep Learning :smile:\"\n",
        ")\n",
        "\n",
        "#cacheamos la URL en caso de que dispongamos de ella\n",
        "if session_state.url is None:\n",
        "    st.write(\"Fist, paste below the predictor server URL: \")\n",
        "else:\n",
        "    st.write(\"Using cached server URL, change if desired:\")\n",
        "\n",
        "server_url = st.text_input(\n",
        "    \"Server URL\", value=(session_state.url if session_state.url is not None else \"\")\n",
        ")\n",
        "session_state.url = server_url\n",
        "\n",
        "if session_state.url is not None and server_url != \"\": # en caso de disponer de ella validamos que el texto se trata de un URL\n",
        "    validate_url(server_url)\n",
        "    predict()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xLTVMjpFNlV",
        "outputId": "ba4cb45d-9f58-42c8-e227-6ad4c9bea80a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting front.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Levantamos el nuevo front:"
      ],
      "metadata": {
        "id": "qFFUtAqyHltS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "conf.get_default().auth_token = \"\"\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(8501)\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZI0eFDzHwFe",
        "outputId": "31a6e5f7-e5ea-43a0-9b28-1d2220c47469"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: http://73cf-104-196-115-8.ngrok.io\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! streamlit run front.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJWKPs_DHlO0",
        "outputId": "9b790383-6e13-4f11-838c-d4ee21d14ecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://104.196.115.8:8501\u001b[0m\n",
            "\u001b[0m\n",
            "2023-03-14 22:01:38.043 `st.cache` is deprecated. Please use one of Streamlit's new caching commands,\n",
            "`st.cache_data` or `st.cache_resource`.\n",
            "\n",
            "More information [in our docs](https://docs.streamlit.io/library/advanced-features/caching).\n",
            "2023-03-14 22:02:40.235 `st.cache` is deprecated. Please use one of Streamlit's new caching commands,\n",
            "`st.cache_data` or `st.cache_resource`.\n",
            "\n",
            "More information [in our docs](https://docs.streamlit.io/library/advanced-features/caching).\n",
            "2023-03-14 22:02:50.180 `st.cache` is deprecated. Please use one of Streamlit's new caching commands,\n",
            "`st.cache_data` or `st.cache_resource`.\n",
            "\n",
            "More information [in our docs](https://docs.streamlit.io/library/advanced-features/caching).\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos pararlo por el momento."
      ],
      "metadata": {
        "id": "X7Xvun1_J5Jl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrySQisyGJKL"
      },
      "source": [
        "# Despliegue en GCP de la aplicación\n",
        "\n",
        "Ahora que ya tenemos nuestro servicio de inferencia online funcionando, estamos listos para desplegar a escala para que soporte miles de peticiones de manera concurrente, para ello utilizaremos el servicio de GCP Cloud Run.\n",
        "\n",
        "Cloud Run es un servicio Serverless (o sin servidor). Serveless nos facilita la puesta en producción de aplicaciones pues, en este caso GCP, se encarga de gestionar la infraestructura y recursos desplegados para nuestra aplicación en base a la carga que tenga esta misma a lo largo del tiempo. \n",
        "\n",
        "Esta práctica nos permita escalar de manera casi infinita, desde dar servicio desde tan solo a decenas de usuarios como a millones de manera concurrente sin tener que realizar ningún ajuste.\n",
        "\n",
        "Las aplicaciones Serverless son una alternativa a los microservicios y los monolitos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9VgEtyHvDQa"
      },
      "source": [
        "## Creando una aplicacion Serverless\n",
        "\n",
        "Para crear nuestra aplicación serverless tan solo nos tenemos que preocupar de que nuestro código funciona y empaquetar todo en una imagen Docker que, finalmente, será lo que despleguemos Cloud Run.\n",
        "\n",
        "Las aplicaciones serverless nos permiten olvidarnos de la gestión de la infrastructura. Basta con implementar el microservicio y Google gestionará toda la infra en torno al mismo.\n",
        "\n",
        "Para crear esta imagen, dado que estamos en un entorno de Google Colab no podemos usar Docker, haremos uso del servicio Cloud Build en GCP, que se encargará de generarnos la imagen con nuestro Dockerfile de la aplicación y finalmente la guardará en el Google Container Registry, donde se almacenan las imágenes Docker. Cloud Build genera un ciclo de integración continua al que podemos enviar trabajos que generan imágenes de Docker."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dU1Fl1SX3IU5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2c20633-e769-4b6e-f333-e2c5f0f27701"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/TwitterOnline\n"
          ]
        }
      ],
      "source": [
        "%cd /content/TwitterOnline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QXKI624uJ-fi",
        "outputId": "c502dd1e-05ac-4633-aba8-e1b67235e127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app  app.py  Dockerfile  model.h5  README.md  requirements.txt\ttokenizer.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subimos el trabajo. Le indicamos la ruta. Coge por defecto el dockerfile y llamas a la imagen con el parámetro que indicamos en el tag."
      ],
      "metadata": {
        "id": "T12Zr1JPJuil"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRhq-d_E7-CY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c7666f4-5dfa-4b14-e5e0-13172220e714"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating temporary tarball archive of 44 file(s) totalling 355.5 MiB before compression.\n",
            "Uploading tarball of [.] to [gs://puestaproduccionkcmarzo23_cloudbuild/source/1678395336.207541-2eac6fbd42d7413b8d426b3ab03b15d9.tgz]\n",
            "Created [https://cloudbuild.googleapis.com/v1/projects/puestaproduccionkcmarzo23/locations/global/builds/b799d99e-8c4d-4e1e-99ce-ddd828a39401].\n",
            "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/b799d99e-8c4d-4e1e-99ce-ddd828a39401?project=1023061570804 ].\n",
            " REMOTE BUILD OUTPUT\n",
            "starting build \"b799d99e-8c4d-4e1e-99ce-ddd828a39401\"\n",
            "\n",
            "FETCHSOURCE\n",
            "Fetching storage object: gs://puestaproduccionkcmarzo23_cloudbuild/source/1678395336.207541-2eac6fbd42d7413b8d426b3ab03b15d9.tgz#1678395357396826\n",
            "Copying gs://puestaproduccionkcmarzo23_cloudbuild/source/1678395336.207541-2eac6fbd42d7413b8d426b3ab03b15d9.tgz#1678395357396826...\n",
            "- [1 files][ 40.8 MiB/ 40.8 MiB]                                                \n",
            "Operation completed over 1 objects/40.8 MiB.\n",
            "BUILD\n",
            "Already have image (with digest): gcr.io/cloud-builders/docker\n",
            "Sending build context to Docker daemon  372.8MB\n",
            "Step 1/4 : FROM tiangolo/uvicorn-gunicorn-fastapi:python3.7\n",
            "python3.7: Pulling from tiangolo/uvicorn-gunicorn-fastapi\n",
            "1e4aec178e08: Already exists\n",
            "6c1024729fee: Pulling fs layer\n",
            "c3aa11fbc85a: Pulling fs layer\n",
            "aa54add66b3a: Pulling fs layer\n",
            "9e3a60c2bce7: Pulling fs layer\n",
            "3b2123ce9d0d: Pulling fs layer\n",
            "bb7d9c78e9c7: Pulling fs layer\n",
            "107978fc70c8: Pulling fs layer\n",
            "74d8ca7b8ce9: Pulling fs layer\n",
            "4bd44dea9300: Pulling fs layer\n",
            "1576ec9a3452: Pulling fs layer\n",
            "29045cbd509f: Pulling fs layer\n",
            "873e652893f4: Pulling fs layer\n",
            "a6763957c32e: Pulling fs layer\n",
            "b2bf28f9833c: Pulling fs layer\n",
            "5f5907f6932d: Pulling fs layer\n",
            "bfcb29a09728: Pulling fs layer\n",
            "63400221381e: Pulling fs layer\n",
            "8ac01cd4a2a8: Pulling fs layer\n",
            "61a02baf4b1e: Pulling fs layer\n",
            "9e3a60c2bce7: Waiting\n",
            "3b2123ce9d0d: Waiting\n",
            "bb7d9c78e9c7: Waiting\n",
            "107978fc70c8: Waiting\n",
            "74d8ca7b8ce9: Waiting\n",
            "4bd44dea9300: Waiting\n",
            "1576ec9a3452: Waiting\n",
            "29045cbd509f: Waiting\n",
            "873e652893f4: Waiting\n",
            "a6763957c32e: Waiting\n",
            "b2bf28f9833c: Waiting\n",
            "5f5907f6932d: Waiting\n",
            "bfcb29a09728: Waiting\n",
            "63400221381e: Waiting\n",
            "8ac01cd4a2a8: Waiting\n",
            "61a02baf4b1e: Waiting\n",
            "6c1024729fee: Verifying Checksum\n",
            "6c1024729fee: Download complete\n",
            "c3aa11fbc85a: Verifying Checksum\n",
            "c3aa11fbc85a: Download complete\n",
            "3b2123ce9d0d: Verifying Checksum\n",
            "3b2123ce9d0d: Download complete\n",
            "bb7d9c78e9c7: Verifying Checksum\n",
            "bb7d9c78e9c7: Download complete\n",
            "aa54add66b3a: Verifying Checksum\n",
            "aa54add66b3a: Download complete\n",
            "107978fc70c8: Verifying Checksum\n",
            "107978fc70c8: Download complete\n",
            "4bd44dea9300: Verifying Checksum\n",
            "4bd44dea9300: Download complete\n",
            "74d8ca7b8ce9: Verifying Checksum\n",
            "74d8ca7b8ce9: Download complete\n",
            "29045cbd509f: Verifying Checksum\n",
            "29045cbd509f: Download complete\n",
            "6c1024729fee: Pull complete\n",
            "873e652893f4: Verifying Checksum\n",
            "873e652893f4: Download complete\n",
            "1576ec9a3452: Verifying Checksum\n",
            "1576ec9a3452: Download complete\n",
            "a6763957c32e: Verifying Checksum\n",
            "a6763957c32e: Download complete\n",
            "b2bf28f9833c: Verifying Checksum\n",
            "b2bf28f9833c: Download complete\n",
            "5f5907f6932d: Verifying Checksum\n",
            "5f5907f6932d: Download complete\n",
            "bfcb29a09728: Verifying Checksum\n",
            "bfcb29a09728: Download complete\n",
            "63400221381e: Verifying Checksum\n",
            "63400221381e: Download complete\n",
            "61a02baf4b1e: Verifying Checksum\n",
            "61a02baf4b1e: Download complete\n",
            "8ac01cd4a2a8: Verifying Checksum\n",
            "8ac01cd4a2a8: Download complete\n",
            "c3aa11fbc85a: Pull complete\n",
            "9e3a60c2bce7: Verifying Checksum\n",
            "9e3a60c2bce7: Download complete\n",
            "aa54add66b3a: Pull complete\n",
            "9e3a60c2bce7: Pull complete\n",
            "3b2123ce9d0d: Pull complete\n",
            "bb7d9c78e9c7: Pull complete\n",
            "107978fc70c8: Pull complete\n",
            "74d8ca7b8ce9: Pull complete\n",
            "4bd44dea9300: Pull complete\n",
            "1576ec9a3452: Pull complete\n",
            "29045cbd509f: Pull complete\n",
            "873e652893f4: Pull complete\n",
            "a6763957c32e: Pull complete\n",
            "b2bf28f9833c: Pull complete\n",
            "5f5907f6932d: Pull complete\n",
            "bfcb29a09728: Pull complete\n",
            "63400221381e: Pull complete\n",
            "8ac01cd4a2a8: Pull complete\n",
            "61a02baf4b1e: Pull complete\n",
            "Digest: sha256:3af65a062020a8a55cbb453b33407b0ead144b80c1cde55573fab442e8bbc84b\n",
            "Status: Downloaded newer image for tiangolo/uvicorn-gunicorn-fastapi:python3.7\n",
            " ---> 99ec71a1ea3f\n",
            "Step 2/4 : COPY ./requirements.txt /requirements.txt\n",
            " ---> b79be4ceb79a\n",
            "Step 3/4 : RUN pip install -r /requirements.txt\n",
            " ---> Running in 7858b585a38b\n",
            "Collecting requests==2.25.0\n",
            "  Downloading requests-2.25.0-py2.py3-none-any.whl (61 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.1/61.1 KB 3.3 MB/s eta 0:00:00\n",
            "Collecting uvicorn==0.12.2\n",
            "  Downloading uvicorn-0.12.2-py3-none-any.whl (45 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.1/45.1 KB 7.1 MB/s eta 0:00:00\n",
            "Collecting fastapi==0.61.2\n",
            "  Downloading fastapi-0.61.2-py3-none-any.whl (48 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.9/48.9 KB 8.0 MB/s eta 0:00:00\n",
            "Collecting h5py==2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.9/2.9 MB 40.6 MB/s eta 0:00:00\n",
            "Collecting scikit-learn==0.23.2\n",
            "  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.8/6.8 MB 43.2 MB/s eta 0:00:00\n",
            "Collecting scipy==1.4.1\n",
            "  Downloading scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.1/26.1 MB 29.9 MB/s eta 0:00:00\n",
            "Collecting tensorflow==2.1.0\n",
            "  Downloading tensorflow-2.1.0-cp37-cp37m-manylinux2010_x86_64.whl (421.8 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 421.8/421.8 MB 2.5 MB/s eta 0:00:00\n",
            "Collecting loguru==0.5.3\n",
            "  Downloading loguru-0.5.3-py3-none-any.whl (57 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.2/57.2 KB 9.1 MB/s eta 0:00:00\n",
            "Collecting gensim==3.6.0\n",
            "  Downloading gensim-3.6.0.tar.gz (23.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.1/23.1 MB 32.5 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting fsspec==0.8.4\n",
            "  Downloading fsspec-0.8.4-py3-none-any.whl (91 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 91.1/91.1 KB 14.0 MB/s eta 0:00:00\n",
            "Collecting gcsfs==0.7.1\n",
            "  Downloading gcsfs-0.7.1-py2.py3-none-any.whl (20 kB)\n",
            "Collecting numpy==1.19.5\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.8/14.8 MB 41.9 MB/s eta 0:00:00\n",
            "Collecting chardet<4,>=3.0.2\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.4/133.4 KB 19.6 MB/s eta 0:00:00\n",
            "Collecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 KB 20.9 MB/s eta 0:00:00\n",
            "Collecting idna<3,>=2.5\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.8/58.8 KB 9.8 MB/s eta 0:00:00\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests==2.25.0->-r /requirements.txt (line 1)) (2022.12.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.7/site-packages (from uvicorn==0.12.2->-r /requirements.txt (line 2)) (0.14.0)\n",
            "Collecting click==7.*\n",
            "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 82.8/82.8 KB 13.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/site-packages (from uvicorn==0.12.2->-r /requirements.txt (line 2)) (4.4.0)\n",
            "Collecting starlette==0.13.6\n",
            "  Downloading starlette-0.13.6-py3-none-any.whl (59 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.0/60.0 KB 9.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pydantic<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/site-packages (from fastapi==0.61.2->-r /requirements.txt (line 3)) (1.10.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from h5py==2.10.0->-r /requirements.txt (line 4)) (1.16.0)\n",
            "Collecting joblib>=0.11\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 298.0/298.0 KB 36.0 MB/s eta 0:00:00\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting tensorboard<2.2.0,>=2.1.0\n",
            "  Downloading tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.8/3.8 MB 67.4 MB/s eta 0:00:00\n",
            "Collecting grpcio>=1.8.6\n",
            "  Downloading grpcio-1.51.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/4.8 MB 54.2 MB/s eta 0:00:00\n",
            "Collecting opt-einsum>=2.3.2\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 KB 11.3 MB/s eta 0:00:00\n",
            "Collecting termcolor>=1.1.0\n",
            "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
            "Collecting google-pasta>=0.1.6\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 KB 10.2 MB/s eta 0:00:00\n",
            "Collecting astor>=0.6.0\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/site-packages (from tensorflow==2.1.0->-r /requirements.txt (line 7)) (0.38.4)\n",
            "Collecting protobuf>=3.8.0\n",
            "  Downloading protobuf-4.22.1-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 302.4/302.4 KB 37.9 MB/s eta 0:00:00\n",
            "Collecting wrapt>=1.11.1\n",
            "  Downloading wrapt-1.15.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.7/75.7 KB 11.9 MB/s eta 0:00:00\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.7/50.7 KB 8.8 MB/s eta 0:00:00\n",
            "Collecting absl-py>=0.7.0\n",
            "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 KB 20.5 MB/s eta 0:00:00\n",
            "Collecting keras-preprocessing>=1.1.0\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.6/42.6 KB 6.6 MB/s eta 0:00:00\n",
            "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "  Downloading tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 449.0/449.0 KB 41.7 MB/s eta 0:00:00\n",
            "Collecting smart_open>=1.2.1\n",
            "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 KB 8.3 MB/s eta 0:00:00\n",
            "Collecting google-auth-oauthlib\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting decorator\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (948 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 948.1/948.1 KB 58.1 MB/s eta 0:00:00\n",
            "Collecting google-auth>=1.2\n",
            "  Downloading google_auth-2.16.2-py2.py3-none-any.whl (177 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 177.2/177.2 KB 25.3 MB/s eta 0:00:00\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 155.3/155.3 KB 21.1 MB/s eta 0:00:00\n",
            "Collecting cachetools<6.0,>=2.0.0\n",
            "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
            "Collecting rsa<5,>=3.1.4\n",
            "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Collecting werkzeug>=0.11.15\n",
            "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.6/233.6 KB 28.4 MB/s eta 0:00:00\n",
            "Collecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 93.3/93.3 KB 14.5 MB/s eta 0:00:00\n",
            "Collecting google-auth>=1.2\n",
            "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 152.9/152.9 KB 22.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r /requirements.txt (line 7)) (57.5.0)\n",
            "Collecting google-auth-oauthlib\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Collecting cachetools<6.0,>=2.0.0\n",
            "  Downloading cachetools-4.2.4-py3-none-any.whl (10 kB)\n",
            "Collecting requests-oauthlib>=0.7.0\n",
            "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Collecting attrs>=17.3.0\n",
            "  Downloading attrs-22.2.0-py3-none-any.whl (60 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.0/60.0 KB 8.8 MB/s eta 0:00:00\n",
            "Collecting charset-normalizer<4.0,>=2.0\n",
            "  Downloading charset_normalizer-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (171 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 171.0/171.0 KB 19.7 MB/s eta 0:00:00\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (148 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 148.0/148.0 KB 21.5 MB/s eta 0:00:00\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 231.4/231.4 KB 30.8 MB/s eta 0:00:00\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94.8/94.8 KB 14.2 MB/s eta 0:00:00\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r /requirements.txt (line 7)) (6.0.0)\n",
            "Collecting pyasn1<0.5.0,>=0.4.6\n",
            "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 KB 11.9 MB/s eta 0:00:00\n",
            "Collecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 KB 21.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r /requirements.txt (line 7)) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0->-r /requirements.txt (line 7)) (3.13.0)\n",
            "Building wheels for collected packages: gensim, gast\n",
            "  Building wheel for gensim (setup.py): started\n",
            "  Building wheel for gensim (setup.py): finished with status 'done'\n",
            "  Created wheel for gensim: filename=gensim-3.6.0-cp37-cp37m-linux_x86_64.whl size=24602085 sha256=f00c24f4ca123a2b448f5081c7c4efdd346e0d68696c84707fba7b148d50df29\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/c8/f9/afb722099bdb5d73e5807019ce1512fd065502ccc15ea2b5bd\n",
            "  Building wheel for gast (setup.py): started\n",
            "  Building wheel for gast (setup.py): finished with status 'done'\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=4c89fc2fd7b9859d7a088354582eb326693c61a816003f5cd765ff8867a4b75a\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gensim gast\n",
            "Installing collected packages: tensorflow-estimator, pyasn1, chardet, wrapt, werkzeug, urllib3, threadpoolctl, termcolor, starlette, smart_open, rsa, pyasn1-modules, protobuf, oauthlib, numpy, multidict, loguru, joblib, idna, grpcio, google-pasta, gast, fsspec, frozenlist, decorator, click, charset-normalizer, cachetools, attrs, asynctest, async-timeout, astor, absl-py, yarl, uvicorn, scipy, requests, opt-einsum, markdown, keras-preprocessing, h5py, google-auth, fastapi, aiosignal, scikit-learn, requests-oauthlib, keras-applications, gensim, aiohttp, google-auth-oauthlib, tensorboard, gcsfs, tensorflow\n",
            "  Attempting uninstall: starlette\n",
            "    Found existing installation: starlette 0.22.0\n",
            "    Uninstalling starlette-0.22.0:\n",
            "      Successfully uninstalled starlette-0.22.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.3\n",
            "    Uninstalling click-8.1.3:\n",
            "      Successfully uninstalled click-8.1.3\n",
            "  Attempting uninstall: uvicorn\n",
            "    Found existing installation: uvicorn 0.20.0\n",
            "    Uninstalling uvicorn-0.20.0:\n",
            "      Successfully uninstalled uvicorn-0.20.0\n",
            "  Attempting uninstall: fastapi\n",
            "    Found existing installation: fastapi 0.88.0\n",
            "    Uninstalling fastapi-0.88.0:\n",
            "      Successfully uninstalled fastapi-0.88.0\n",
            "Successfully installed absl-py-1.4.0 aiohttp-3.8.4 aiosignal-1.3.1 astor-0.8.1 async-timeout-4.0.2 asynctest-0.13.0 attrs-22.2.0 cachetools-4.2.4 chardet-3.0.4 charset-normalizer-3.1.0 click-7.1.2 decorator-5.1.1 fastapi-0.61.2 frozenlist-1.3.3 fsspec-0.8.4 gast-0.2.2 gcsfs-0.7.1 gensim-3.6.0 google-auth-1.35.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.51.3 h5py-2.10.0 idna-2.10 joblib-1.2.0 keras-applications-1.0.8 keras-preprocessing-1.1.2 loguru-0.5.3 markdown-3.4.1 multidict-6.0.4 numpy-1.19.5 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.22.1 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.25.0 requests-oauthlib-1.3.1 rsa-4.9 scikit-learn-0.23.2 scipy-1.4.1 smart_open-6.3.0 starlette-0.13.6 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0 termcolor-2.2.0 threadpoolctl-3.1.0 urllib3-1.26.14 uvicorn-0.12.2 werkzeug-2.2.3 wrapt-1.15.0 yarl-1.8.2\n",
            "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "\u001b[0m\u001b[91mWARNING: You are using pip version 22.0.4; however, version 23.0.1 is available.\n",
            "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\n",
            "\u001b[0mRemoving intermediate container 7858b585a38b\n",
            " ---> 3c602b1869d8\n",
            "Step 4/4 : COPY ./app /app/app\n",
            " ---> 657b19645395\n",
            "Successfully built 657b19645395\n",
            "Successfully tagged gcr.io/puestaproduccionkcmarzo23/sentiment-analysis-server:latest\n",
            "PUSH\n",
            "Pushing gcr.io/puestaproduccionkcmarzo23/sentiment-analysis-server\n",
            "The push refers to repository [gcr.io/puestaproduccionkcmarzo23/sentiment-analysis-server]\n",
            "84b49d995507: Preparing\n",
            "e4462816196e: Preparing\n",
            "5d9f8c559e90: Preparing\n",
            "8cfb63353c56: Preparing\n",
            "8429bedecefd: Preparing\n",
            "f208e1a6fd33: Preparing\n",
            "43f2732d8fa7: Preparing\n",
            "8dc483c072e6: Preparing\n",
            "4c3e5f78c8ec: Preparing\n",
            "82342eba7feb: Preparing\n",
            "23655854d429: Preparing\n",
            "49f7d4839ca6: Preparing\n",
            "21f182fedf3e: Preparing\n",
            "7ab2417b9367: Preparing\n",
            "4169ad63f09d: Preparing\n",
            "b2cc056dd71d: Preparing\n",
            "2167aebfcb85: Preparing\n",
            "17efc79ac0fe: Preparing\n",
            "0b6859e9fff1: Preparing\n",
            "11829b3be9c0: Preparing\n",
            "dc8e1d8b53e9: Preparing\n",
            "9d49e0bc68a4: Preparing\n",
            "8e396a1aad50: Preparing\n",
            "21f182fedf3e: Waiting\n",
            "7ab2417b9367: Waiting\n",
            "4169ad63f09d: Waiting\n",
            "b2cc056dd71d: Waiting\n",
            "2167aebfcb85: Waiting\n",
            "17efc79ac0fe: Waiting\n",
            "0b6859e9fff1: Waiting\n",
            "11829b3be9c0: Waiting\n",
            "dc8e1d8b53e9: Waiting\n",
            "9d49e0bc68a4: Waiting\n",
            "8e396a1aad50: Waiting\n",
            "8dc483c072e6: Waiting\n",
            "4c3e5f78c8ec: Waiting\n",
            "82342eba7feb: Waiting\n",
            "23655854d429: Waiting\n",
            "49f7d4839ca6: Waiting\n",
            "f208e1a6fd33: Waiting\n",
            "43f2732d8fa7: Waiting\n",
            "8429bedecefd: Layer already exists\n",
            "8cfb63353c56: Layer already exists\n",
            "f208e1a6fd33: Layer already exists\n",
            "43f2732d8fa7: Layer already exists\n",
            "8dc483c072e6: Layer already exists\n",
            "4c3e5f78c8ec: Layer already exists\n",
            "82342eba7feb: Layer already exists\n",
            "23655854d429: Layer already exists\n",
            "49f7d4839ca6: Layer already exists\n",
            "21f182fedf3e: Layer already exists\n",
            "84b49d995507: Pushed\n",
            "5d9f8c559e90: Pushed\n",
            "7ab2417b9367: Layer already exists\n",
            "4169ad63f09d: Layer already exists\n",
            "b2cc056dd71d: Layer already exists\n",
            "17efc79ac0fe: Layer already exists\n",
            "2167aebfcb85: Layer already exists\n",
            "0b6859e9fff1: Layer already exists\n",
            "11829b3be9c0: Layer already exists\n",
            "9d49e0bc68a4: Layer already exists\n",
            "8e396a1aad50: Layer already exists\n",
            "dc8e1d8b53e9: Layer already exists\n",
            "e4462816196e: Pushed\n",
            "latest: digest: sha256:916642ae2e90150954e82746a2523b3df244f8f18fdcba571398b3a364f93ace size: 5134\n",
            "DONE\n",
            "\n",
            "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                   IMAGES                                                                STATUS\n",
            "b799d99e-8c4d-4e1e-99ce-ddd828a39401  2023-03-09T20:55:59+00:00  5M37S     gs://puestaproduccionkcmarzo23_cloudbuild/source/1678395336.207541-2eac6fbd42d7413b8d426b3ab03b15d9.tgz  gcr.io/puestaproduccionkcmarzo23/sentiment-analysis-server (+1 more)  SUCCESS\n"
          ]
        }
      ],
      "source": [
        "! gcloud builds submit --tag gcr.io/$PROJECT_ID/sentiment-analysis-server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bJFhmAjEvQx"
      },
      "source": [
        "Finalmente, procederemos a desplegar la imagen docker en el servicio de GCP Cloud Run.\n",
        "\n",
        "Para ello buscamos Cloud Run en las herramientas de Cloud, habilitamos la API. Vamos a crear servicio. Elegimos la imagen docker que hemos construido. Le damos un nombre e indicamos donde desplegarlo.\n",
        "\n",
        "En memoria ponemos 8GB con 4 núcleos.\n",
        "\n",
        "Marcamos primera generación en entoros de ejecución.\n",
        "\n",
        "En variables de entorno agregamos: DEFAULT_MODEL_PATH : gs://content/TwitterOnline/data/model\n",
        "\n",
        "Una vez levantado nos va a generar una URL.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oZ75Cy498tg"
      },
      "source": [
        "## Probando nuestra aplicación desplegada\n",
        "\n",
        "Una vez desplegado nuestra aplicación serverless de inferencia online... ¡Podemos usarla desde cualquier lugar del planeta! Tanto si es un usuario como si son millones. Para hacer un ejemplo de petición a nuestro modelo desplegado, podemos hacer lo siguiente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KC0honui-W23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0d62d5f-d0b1-4763-e5a1-8c83546d780a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Service Unavailable"
          ]
        }
      ],
      "source": [
        "! curl -X POST \"https://sentiment-analysis-server-xj4lbza6gq-ew.a.run.app/api/model/predict\" -H  \"accept: application/json\" -H  \"Content-Type: application/json\" -d \"{\\\"text\\\":\\\"i hate\\\"}\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj8bcxFX85UO"
      },
      "source": [
        "# Práctica Final: detección de mensajes troll \n",
        "\n",
        "En los últimos años Twitch se ha consolidad como uno de los principales medios de comunicación especialmente para las generaciones más jóvenes.\n",
        "\n",
        "Al tratarse de una plataforma participativa en la que los usuarios pueden poner comentarios durante y posteriormente a las emisiones. Entre estos comentarios han aparecido como siempre comentarios ofensisvos.\n",
        "\n",
        "En esta práctica construiremos una Inteligencia Artificial capaz de clasificar esos mensajes troll.\n",
        "\n",
        "Durante la práctica entrenaremos el modelo de Deep Learning y lo desplegaremos para inferencia en batch, la más habitual actualmente dentro de la industria:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teoría\n",
        "\n",
        "1. ¿Qué es Apache Beam?\n",
        "\n",
        "Sirve para preparar pipelines para realizar procesamiento de datos, diseñado para ejecutar en varios motores de procesamiento, como Apache Spark, Apache Flink, Google Cloud Dataflow...\n",
        "Beam te permite preparar el proceso para ejecutarlo de la misma forma sin importar el entorno en el que se ejecute.\n",
        "\n",
        "2. ¿Cuáles son las diferentes formas de desplegar un modelo?\n",
        "\n",
        "Se puede desplegar en local (propia máquina), en un servidor On Premise (servidor propio físico) o despliegue en la nube (Google Cloud, AWS, Azure...)\n",
        "\n",
        "3. ¿Cuál es la principal diferencia entre la inferencia en batch y la inferencia en streaming?\n",
        "\n",
        "El batch se procesa por lotes, en paralelo y en grandes cantidades, no es en tiempo real. En streaming se procesan según llegan en tiempo real, se hace una prediccion por cada paquete de datos y se devuelve el resultado en el mismo momento."
      ],
      "metadata": {
        "id": "AI5vlvwU9FAv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgE9XB8ACVmQ"
      },
      "source": [
        "# Configuración de nuestro proyecto en GCP\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAorVw7RCT9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62c248cc-cad3-4818-fc1d-cd012202007f"
      },
      "source": [
        "PROJECT_ID = \"deploypractice-380919\" #@param {type:\"string\"}\n",
        "! gcloud config set project $PROJECT_ID"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbAl8pSkCXCm"
      },
      "source": [
        "import sys\n",
        "\n",
        "# If you are running this notebook in Colab, run this cell and follow the\n",
        "# instructions to authenticate your GCP account. This provides access to your\n",
        "# Cloud Storage bucket and lets you submit training jobs and prediction\n",
        "# requests.\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "  from google.colab import auth as google_auth\n",
        "  google_auth.authenticate_user()\n",
        "\n",
        "# If you are running this notebook locally, replace the string below with the\n",
        "# path to your service account key and run this cell to authenticate your GCP\n",
        "# account.\n",
        "else:\n",
        "  %env GOOGLE_APPLICATION_CREDENTIALS ''\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos el bucket mediante la consola y una vez creado lo indicamos en la variable:"
      ],
      "metadata": {
        "id": "p5Xoej7vH0X-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6Zpx14FCfXy"
      },
      "source": [
        "BUCKET_NAME = \"bucket-practice-armando\" #@param {type:\"string\"}\n",
        "REGION = \"europe-west1\" #@param {type:\"string\"}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE4V-Xt0ChSb"
      },
      "source": [
        "! gsutil ls -al gs://$BUCKET_NAME"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D8Qw14y_nRa"
      },
      "source": [
        "# Entrenamiento e inferencia en Batch\n",
        "\n",
        "## Preparación\n",
        "\n",
        "Para esta primera parte se va a utilizar [Tweets Dataset for Detection of Cyber-Trolls](https://www.kaggle.com/dataturks/dataset-for-detection-of-cybertrolls). El objetivo es desarrollar un clasificador binario para detectar si el mensaje recibido es troll (1) o no (0). **Las métricas obtenidas del entrenamiento y la inferencia no se tendrán en cuenta para la evaluación de la práctica, la importancia está en la arquitectura de la solución**, es decir, lo importante no es que nuestro modelo detecte correctamente los tweets de trolls si no que funcione y sea capaz de hacer inferencias.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM_dh-471gIq"
      },
      "source": [
        "A continuación, se van a subir los datos de entrenamiento al bucket del proyecto que se haya creado. **Importante:** crea el bucket en una única región. Os dejo disponibilizado el dataset en un bucket de acceso público:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqp4L_nmUjAc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d14a072e-5e20-49ae-fc21-859bc3efb3d0"
      },
      "source": [
        "# Upload data to your bucket\n",
        "! wget https://storage.googleapis.com/datos-practica-compartidos/Dataset%20for%20Detection%20of%20Cyber-Trolls.json -O - | gsutil cp - gs://$BUCKET_NAME/data.json"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-17 19:35:43--  https://storage.googleapis.com/datos-practica-compartidos/Dataset%20for%20Detection%20of%20Cyber-Trolls.json\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.12.128, 172.217.194.128, 74.125.200.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.12.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2756023 (2.6M) [application/json]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                     0%[                    ]       0  --.-KB/s               Copying from <STDIN>...\n",
            "-                   100%[===================>]   2.63M   514KB/s    in 5.2s    \n",
            "\n",
            "2023-03-17 19:35:49 (514 KB/s) - written to stdout [2756023/2756023]\n",
            "\n",
            "/ [1 files][    0.0 B/    0.0 B]                                                \n",
            "Operation completed over 1 objects.                                              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCQQ9j2I11tg"
      },
      "source": [
        "Ahora se crea el directorio dónde vas a desarrollar esta primera parte de la práctica."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsblBlJ6RrGm"
      },
      "source": [
        "%mkdir /content/batch"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyK51quU1_oi"
      },
      "source": [
        "Se establece el directorio de trabajo que hemos creado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7ybSaotRwkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88aa6a36-2b12-46df-87d3-fbf3ebf686f4"
      },
      "source": [
        "import os\n",
        "\n",
        "# Set the working directory to the sample code directory\n",
        "%cd /content/batch\n",
        "\n",
        "WORK_DIR = os.getcwd()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/batch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NSd4bAo2aj_"
      },
      "source": [
        "Ahora se descargarán los datos en el workspace de Colab para trabajar en local."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOmUJSg1JCgg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2889afe-0243-4dfa-b764-777416f1a63e"
      },
      "source": [
        "! wget https://storage.googleapis.com/datos-practica-compartidos/Dataset%20for%20Detection%20of%20Cyber-Trolls.json"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-17 19:36:22--  https://storage.googleapis.com/datos-practica-compartidos/Dataset%20for%20Detection%20of%20Cyber-Trolls.json\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.200.128, 74.125.68.128, 74.125.24.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.200.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2756023 (2.6M) [application/json]\n",
            "Saving to: ‘Dataset for Detection of Cyber-Trolls.json’\n",
            "\n",
            "\r          Dataset f   0%[                    ]       0  --.-KB/s               \rDataset for Detecti 100%[===================>]   2.63M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-03-17 19:36:22 (117 MB/s) - ‘Dataset for Detection of Cyber-Trolls.json’ saved [2756023/2756023]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRoW6zyg2kEz"
      },
      "source": [
        "Se establecen las dependencias que se usarán en la práctica. Se pueden añadir y quitar las dependencias que no se usen o viceversa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s0-8_JK_z2D",
        "outputId": "a6d26fd3-e670-48be-dd37-1cef72934722",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile requirements.txt\n",
        "\n",
        "apache-beam[gcp]==2.24.0\n",
        "tensorflow\n",
        "gensim==3.6.0\n",
        "fsspec==0.8.4\n",
        "gcsfs==0.7.1\n",
        "numpy==1.20.0\n",
        "apache-beam\n",
        "pyarrow==0.17.1"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIl8lrPp2x2j"
      },
      "source": [
        "Instalamos las dependencias. **No olvides reiniciar el entorno al instalar y establecer las variables y credenciales de GCP al arrancar.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZS6P8A4c_8le",
        "outputId": "9986cccd-097e-405f-b4b5-dc4aba12d7d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! pip install -r requirements.txt"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting apache-beam[gcp]==2.24.0\n",
            "  Downloading apache-beam-2.24.0.zip (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 3)) (2.11.0)\n",
            "Requirement already satisfied: gensim==3.6.0 in /usr/local/lib/python3.9/dist-packages (from -r requirements.txt (line 4)) (3.6.0)\n",
            "Collecting fsspec==0.8.4\n",
            "  Downloading fsspec-0.8.4-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 KB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gcsfs==0.7.1\n",
            "  Downloading gcsfs-0.7.1-py2.py3-none-any.whl (20 kB)\n",
            "Collecting numpy==1.20.0\n",
            "  Downloading numpy-1.20.0-cp39-cp39-manylinux2010_x86_64.whl (15.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting apache-beam\n",
            "  Downloading apache_beam-2.46.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyarrow==0.17.1\n",
            "  Downloading pyarrow-0.17.1.tar.gz (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0DDu27S3CJv"
      },
      "source": [
        "## Primer ejercicio\n",
        "\n",
        "Desarrollar un pipeline de preprocesamiento utilizando Apache Beam para generar datos de train, eval y test para los datos proporcionados anteriormente. Requisitos:\n",
        "\n",
        "- Proporcionar dos modos de ejecución: `train` y `test`\n",
        "- Soportar ejecuciones en local con `DirectRunner` y ejecuciones en Dataflow usando `DataFlowRunner`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1blctlxs_dPO"
      },
      "source": [
        "%%writefile preprocess.py\n",
        "\n",
        "# Rellena con tu solución\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qawuKC0k4B0e"
      },
      "source": [
        "Se proporciona un fichero `setup.py` necesario para ejecutar en DataFlow. Modificar la variable `REQUIRED_PACKAGES` con las dependencias que se hayan usado en el `requirements.txt`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1MQvWsk_mVX"
      },
      "source": [
        "%%writefile setup.py\n",
        "\n",
        "import setuptools\n",
        "\n",
        "REQUIRED_PACKAGES = [\n",
        "  \"apache-beam[gcp]==2.24.0\",\n",
        "  \"tensorflow==2.8.0\"\n",
        "]\n",
        "\n",
        "setuptools.setup(\n",
        "    name=\"twitchstreaming\",\n",
        "    version=\"0.0.1\",\n",
        "    install_requires=REQUIRED_PACKAGES,\n",
        "    packages=setuptools.find_packages(),\n",
        "    include_package_data=True,\n",
        "    description=\"Troll detection\",\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLDlz_fL4UJA"
      },
      "source": [
        "### Validación preprocess train en local\n",
        "Con el comando mostrado a continuación se valida la correcta generación de los datos de entrenamiento y validación en local."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFpirg37C3bN"
      },
      "source": [
        "! python3 preprocess.py \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --runner DirectRunner \\\n",
        "  --input $WORK_DIR/data.json \\\n",
        "  --output $WORK_DIR/transformed_data \\\n",
        "  --mode train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkkG271a4qnA"
      },
      "source": [
        "### Validación preprocess test en local \n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta generación de los datos de test en local."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4lkLgecLS3i"
      },
      "source": [
        "! python3 preprocess.py \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --runner DirectRunner \\\n",
        "  --input $WORK_DIR/data.json \\\n",
        "  --output $WORK_DIR/transformed_data \\\n",
        "  --mode test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geZM9Sbj45LK"
      },
      "source": [
        "## Segundo ejercicio\n",
        "\n",
        "Desarrollar una tarea de entrenamiento para los datos preprocesados. Requisitos:\n",
        "\n",
        "- Soportar ejecuciones en local usando el SDK de AI-Platform y ejecuciones en GCP con el mismo código."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMUwXgm_5el-"
      },
      "source": [
        "Se crea el directorio donde trabajaremos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMi8dI1gLoIc"
      },
      "source": [
        "%mkdir /content/batch/trainer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dJyMXTuNPwo"
      },
      "source": [
        "%%writefile trainer/__init__.py\n",
        "\n",
        "version = \"0.1.0\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUu6gKaTL_S2"
      },
      "source": [
        "%%writefile trainer/task.py\n",
        "\n",
        "# Rellena con tu solución"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8axPHdHX6d9W"
      },
      "source": [
        "### Validación Train en local\n",
        "\n",
        "Con el comando mostrado a continuación se valida el correcto entrenamiento del modelo usando los datos preprocesados del apartado anterior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9997ZzsLmq-"
      },
      "source": [
        "# Explicitly tell `gcloud ai-platform local train` to use Python 3 \n",
        "! gcloud config set ml_engine/local_python $(which python3)\n",
        "\n",
        "# This is similar to `python -m trainer.task --job-dir local-training-output`\n",
        "# but it better replicates the AI Platform environment, especially for\n",
        "# distributed training (not applicable here).\n",
        "! gcloud ai-platform local train \\\n",
        "  --package-path trainer \\\n",
        "  --module-name trainer.task \\\n",
        "  -- \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --epochs 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nhlkcjn62Zo"
      },
      "source": [
        "## Tercer ejercicio\n",
        "\n",
        "Desarrollar un pipeline de inferencia utilizando Apache Beam para generar predicciones usando los modelos generados en el apartado anterior así como los datos de test generados en el primer ejercicio.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHBpLXB-OmjT"
      },
      "source": [
        "%%writefile predict.py\n",
        "\n",
        "# Rellena con tu solución"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUdgcLIP7a42"
      },
      "source": [
        "Generamos un timestamp para la ejecución de las predicciones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pvKMtuQPOlr"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# current date and time\n",
        "TIMESTAMP = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkLMdMup70ne"
      },
      "source": [
        "### Validación Predict en local\n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta inferencia usando los modelos anteriores y los datos de test generados anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUJN0XCLPR_X"
      },
      "source": [
        "! python3 predict.py \\\n",
        "  --work-dir $WORK_DIR \\\n",
        "  --model-dir $WORK_DIR/model \\\n",
        "  batch \\\n",
        "  --inputs-dir $WORK_DIR/transformed_data/test/part* \\\n",
        "  --outputs-dir $WORK_DIR/predictions/$TIMESTAMP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Um68jx_F8mjF"
      },
      "source": [
        "## Extra. Comprobaciones en la nube\n",
        "\n",
        "En esta última parte se validará el funcionamiento del código en un proyecto de GCP sobre DataFlow y AI Platform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JHC9dCT83FH"
      },
      "source": [
        "Establecemos el bucket y region de GCP sobre el que trabajaremos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgUpX__KQvt-"
      },
      "source": [
        "GCP_WORK_DIR = ''\n",
        "GCP_REGION = ''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_A66WAfm9DmU"
      },
      "source": [
        "### Validación preprocess train en Dataflow\n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta generación de los datos de entrenamiento y validación en GCP con el servicio DataFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJYVuWyJP1Ya"
      },
      "source": [
        "! python3 preprocess.py \\\n",
        "  --project $PROJECT_ID \\\n",
        "  --region $GCP_REGION \\\n",
        "  --runner DataflowRunner \\\n",
        "  --temp_location $GCP_WORK_DIR/beam-temp \\\n",
        "  --setup_file ./setup.py \\\n",
        "  --work-dir $GCP_WORK_DIR \\\n",
        "  --input $GCP_WORK_DIR/data.json \\\n",
        "  --output $GCP_WORK_DIR/transformed_data \\\n",
        "  --mode train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8czKRMb99ZRX"
      },
      "source": [
        "### Validación preprocess test en Dataflow\n",
        "\n",
        "Con el comando mostrado a continuación se valida la correcta generación de los datos de test en GCP con el servicio DataFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45mfSRGdQLBO"
      },
      "source": [
        "! python3 preprocess.py \\\n",
        "  --project $PROJECT_ID \\\n",
        "  --region $GCP_REGION \\\n",
        "  --runner DataflowRunner \\\n",
        "  --temp_location $GCP_WORK_DIR/beam-temp \\\n",
        "  --setup_file ./setup.py \\\n",
        "  --work-dir $GCP_WORK_DIR \\\n",
        "  --input $GCP_WORK_DIR/data.json \\\n",
        "  --output $GCP_WORK_DIR/transformed_data \\\n",
        "  --mode test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Omitimos el entrenamiento en la nube para no incurrir en costes innecesarios."
      ],
      "metadata": {
        "id": "sTVj6bqII_3m"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu5SOWsy9886"
      },
      "source": [
        "### Validación predict en Dataflow\n",
        "\n",
        "Con el comando mostrado a continuación se valida la predicción correcta de los datos de test usando los modelos generados en el comando anterior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXoHCgMg-LUD"
      },
      "source": [
        "Generamos un timestamp para el almacenamiento de las inferencias en Google Cloud Storage."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmcetQtnQjVo"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# current date and time\n",
        "TIMESTAMP = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT_NxsKDQlxD"
      },
      "source": [
        "# For using sample models: --model-dir gs://$BUCKET_NAME/models/\n",
        "! python3 predict.py \\\n",
        "  --work-dir $GCP_WORK_DIR \\\n",
        "  --model-dir $GCP_WORK_DIR/model/ \\\n",
        "  batch \\\n",
        "  --project $PROJECT_ID \\\n",
        "  --region $GCP_REGION \\\n",
        "  --runner DataflowRunner \\\n",
        "  --temp_location $GCP_WORK_DIR/beam-temp \\\n",
        "  --setup_file ./setup.py \\\n",
        "  --inputs-dir $GCP_WORK_DIR/transformed_data/test/part* \\\n",
        "  --outputs-dir $GCP_WORK_DIR/predictions/$TIMESTAMP"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}